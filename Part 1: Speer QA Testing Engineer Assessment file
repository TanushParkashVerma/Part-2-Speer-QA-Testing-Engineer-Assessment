var request = require('request');
var cheerio = require('cheerio');
var URL = require('url-parse');

var START_URL = "https://en.wikipedia.org/wiki/Main_Page";
var SEARCH_WORD = "stemming";
var MAX_PAGES_TO_VISIT = 10;

var pagesVisited = {};
var numPagesVisited = 0;
var pagesToVisit = [];
var url = new URL(START_URL);
var baseUrl = url.protocol + "//" + url.hostname;

pagesToVisit.push(START_URL);
crawl();

function crawl() {
  if(numPagesVisited >= MAX_PAGES_TO_VISIT) {
     console.log("Reached max limit of number of pages to visit.");
     return;
  }
  var nextPage = pagesToVisit.pop();
  if (nextPage in pagesVisited) {
     // We've already visited this page, so repeat the crawl
     crawl();
  } else {
     // New page we haven't visited
     visitPage(nextPage, crawl);
  }
}

function visitPage(url, callback) {
  // Add page to our set
  pagesVisited[url] = true;
  numPagesVisited++;

  // Make the request
  console.log("Visiting page " + url);
  request(url, function(error, response, body) {
     // Check status code (200 is HTTP OK)
     console.log("Status code: " + response.statusCode);
     if(response.statusCode !== 200) {
        callback();
        return;
     }
     // Parse the document body
     var $ = cheerio.load(body);
     var isWordFound = searchForWord($, SEARCH_WORD);
     if(isWordFound) {
        console.log('Word ' + SEARCH_WORD + ' found at page ' + url);
     } else {
        collectInternalLinks($);
        // In this short program, our callback is just calling crawl()
        callback();
     }
  });
}

function searchForWord($, word) {
  var bodyText = $('html > body').text().toLowerCase();
  return(bodyText.indexOf(word.toLowerCase()) !== -1);
}

function collectInternalLinks($) {
   var relativeLinks = $("a[href^='/']");
   console.log("Found " + relativeLinks.length + " relative links on page");
   relativeLinks.each(function() {
       pagesToVisit.push(baseUrl + $(this).attr('href'));
   });
}

Explanation:

-The given Java script is a web crawler that starts from a given URL (START_URL) and visits
pages in a breadth first manner until a given number of pages (MAX_PAGES_TO_VISIT) has been
reached. For each page, the script will check if a given word (SEARCH_WORD) is present. If
the word is found, the script will print a message to the console. If the word is not found,
the script will collect all internal links present on the page and add them to a list of pages
to visit (pagesToVisit). The script will then visit the next page in the list.

-This process will continue until either the SEARCH_WORD is found or the MAX_PAGES_TO_VISIT
has been reached.One potential issue with this script is that it will continue to add pages to the list of pages to visit even if the SEARCH_WORD has already been found. This could lead to the script
visiting more pages than necessary.

-Another potential issue is that the script does not keep track of which links have already been
visited. This could lead to the script visiting the same page multiple times which would be
wasteful. Overall, this script is a simple web crawler that could be used to crawl a website and check
for the presence of a given word. However, it is not very efficient and could be improved.
